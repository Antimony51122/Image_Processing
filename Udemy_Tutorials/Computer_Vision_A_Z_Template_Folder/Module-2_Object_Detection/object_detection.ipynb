{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries\n",
    "\n",
    "- **`torch`** is by far the best weapon to build neural network and computer vision because pytorch contains the dynamic graphs which we are able to compute very efficiently the gradients of composition functions in backward propagation. \n",
    "- **`autograd`** is the module responsible for gradient descent. We are import the variable class which will be used to convert the tensors into some torch variables that will contain both the tensor and a gradient and then the torch variable containing the tensor in the gradients will be on element of the graph.\n",
    "- **`data`** is a folder that contains the classes `BaseTransform` and `VOC_CLASSES`: \n",
    "    - `BaseTransform` is a class that will do the required transformations so that the input images will be compatible with the neural network. (When we eed the neural network with the input images, they have to have a certain format and `BaseTransform` will be used to transform the images in this format so that they can be accepted into the neural network)\n",
    "    - `VOC_CLASSES` is a dictionary that will do the encoding of the classes, (for example: planes will be encoded as one) which is the idea of doing a mapping because we want to work with numbers and not text.\n",
    "- **`SSD`** is the library of the single shot multi-box action model and then `build_ssd` as that we import from the `SSD` library will be the constructor the architecture of single shot (not box detection) of the SSD neural network.\n",
    "- **`imageio`** is the library that we will use to process the images of the video and applying the detect function that will implement on the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "from data import BaseTransform, VOC_CLASSES as labelmap\n",
    "from ssd import build_ssd\n",
    "import imageio\n",
    "imageio.plugins.ffmpeg.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing single-shot multi-box detection through deep learning instead of OpenCV\n",
    "\n",
    "- The function will be working frame by frame. Instead of doing directly on the video, it will do on each single image.\n",
    "- Using tricks from `imageio` to extract images from the video and implement interesting functions.\n",
    "- Reassemble the whole thing to make the video with the rectangles indicating detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we are going to do several transformations to go from the original image to a torch variable that will be accepted into the SSD neural network. There is a series of transformations to do before getting to this torch variable.\n",
    "1. apply the transform transformation to make sure that the image has the \n",
    "    - right format, \n",
    "    - right dimensions \n",
    "    - right colour values\n",
    "2. once done this transformation, we need to convert this transformed frame from a **numpy array** to a **torch tensor** (a tensor is a more advanced matrix)\n",
    "3. add a fake dimension to the torch sensor and that fake dimension will correspond to the batch\n",
    "4. convert into a torch variable that contains both the tensor and the gradient. (The torch variable will be an element of the dynamic graph which will allow us later to do fast adn efficient computation of the gradients during backward propogation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First transformation\n",
    "\n",
    "`transform()` function returns two elements but we are only interested in the first element which is actually the reansform frame with the right format. In order to get the first element, we add index function which is `[0]`:\n",
    "\n",
    "`frame_t = transform(frame)[0]`\n",
    "\n",
    "#### Second transformation\n",
    "\n",
    "- Changing numpy array to torch tensor: `x = torch.from_numpy(frame_t)`\n",
    "\n",
    "- The output is RBG but the neural network training is in GRB, needs simple bit transformation: `x = torch.from_numpy(frame_t).permute(2, 0, 1)`\n",
    "\n",
    "#### Third transformation\n",
    "\n",
    "The neural network cannot actually accept single inputs like a single input vector or a single input image. It only accepts them into some batches. That's why we have to create a structure with the first dimension corresponding to the batch and the other dimension corresponding to the input.\n",
    "\n",
    "`x.unsqueeze(0)`\n",
    "\n",
    "#### Fourth transformation\n",
    "\n",
    "Convert the batch of torch and input into a torch variable (which is a highly advanced variable contains both a tensor and a gradient). This torch variable will become an element of the dynamic graph which will compute very efficiently the gradients of any composition functions during backward propagation.\n",
    "\n",
    "Variable class will create an object which will be the torch variable and therefore since we are creating a new object we need to overwrite the previous variable x:\n",
    "\n",
    "`x = Variable(x.unsqueeze(0))`\n",
    "\n",
    "Then it's ready to be fed into the SSD neural network that has been pre-trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because the position of the detected objects inside the image has to be normailsed between 0 and 1, to do this normalisation, we will need this scale tensor with these four dimensions. \n",
    "\n",
    "`scale = torch.Tensor([width, height, width, height])`\n",
    "\n",
    "The reason of double width height is that the first two width height will correspond to the scale of values of the upper left corner of the rectangle detector and the second width height will correspond to the scale of values of the lower right corner of this same retangle detector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that will do the detections\n",
    "# net is the neural network\n",
    "# transform makes sure image compatible with the neural network\n",
    "def detect(frame, net, transform):\n",
    "    height, width = frame.shape[:2]\n",
    "    frame_t = transform(frame)[0]  # correspond to the future transform frame\n",
    "    x = torch.from_numpy(frame_t).permute(2, 0, 1)  # numpy array to torch tensor; RBG to GRB\n",
    "    x = Variable(x.unsqueeze(0))  # add fake dimension corresponding to the batch\n",
    "    y = net(x)  # feed x to the neural network\n",
    "    detections = y.data\n",
    "    scale = torch.Tensor([width, height, width, height])\n",
    "    # detections = [batch, number of classes, number of occurence, (score, x0, y0, x1, y1)]\n",
    "    for i in range(detections.size(1)):\n",
    "        j = 0  # occurence of a class\n",
    "        # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\n",
    "        while detections[0, i, j, 0] >= 0.6:\n",
    "            # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\n",
    "            pt = (detections[0, i, j, 1:] * scale).numpy()\n",
    "            # We draw a rectangle around the detected object.\n",
    "            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2)\n",
    "            # We put the label of the class right above the rectangle.\n",
    "            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            j += 1  # We increment j to get to the next occurrence.\n",
    "    # We return the original frame with the detector rectangle and the label around the detected object.\n",
    "    return frame  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the SSD neural network\n",
    "net = build_ssd('test')\n",
    "net.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage))  \n",
    "# not only we have a tensor that contains these weights but also these weights are attributed to our SSD net object\n",
    "\n",
    "# Creating the transformation (make the image compatible with the neural network)\n",
    "transform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rh3014/coding/Image_Processing/Udemy_Tutorials/Computer_Vision_A_Z_Template_Folder/Module-2_Object_Detection/ssd.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  self.softmax(conf.view(-1, self.num_classes)),  # conf preds\n",
      "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1080, 1920) to (1088, 1920) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "# Doing some Object Detection on a video\n",
    "reader = imageio.get_reader('./videos/funny_dog.mp4')\n",
    "fps = reader.get_meta_data()['fps']  # get the fps frequence (frames per second).\n",
    "writer = imageio.get_writer('./videos/output.mp4', fps = fps)  # create an output video with this same fps frequence.\n",
    "for i, frame in enumerate(reader):  # iterate on the frames of the output video\n",
    "    frame = detect(frame, net.eval(), transform)\n",
    "    writer.append_data(frame)  # add the next frame in the output video.\n",
    "    print(i)  # print the number of the processed frame.\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/scene1_dog_personFront.png\">\n",
    "\n",
    "> - In this frame, the guy with red shirt in the front and the dog has been detected. \n",
    "> - The people behind the person in red has been covered by his hand thus not detected.\n",
    "\n",
    "<img src=\"./images/scene2_dogMergePerson.png\">\n",
    "\n",
    "> - In this frame, the dog has covered half of the man in the back, thus the computer regarded them as one body and merged them together as one person"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
